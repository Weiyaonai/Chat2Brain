{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess_brain_map"
      ],
      "metadata": {
        "id": "R2MY4sDqsv6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q neuroquery\n",
        "!pip install -q nilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tq5z8SqtDop",
        "outputId": "3a6d3f80-2ad8-4555-db9d-8e0eefe2d393"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1klPtXRrtKe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GeddlOJ7sfp3"
      },
      "outputs": [],
      "source": [
        "import neuroquery\n",
        "import nilearn\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from joblib import Memory\n",
        "from nilearn import image\n",
        "from neuroquery._compat import maskers, load_mni152_brain_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "coord2map"
      ],
      "metadata": {
        "id": "n4_NpxkGsJ4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_masker(mask_img=None, target_affine=None):\n",
        "    if isinstance(mask_img, maskers.NiftiMasker):\n",
        "        return mask_img\n",
        "    if mask_img is None:\n",
        "        mask_img = load_mni152_brain_mask()\n",
        "    if target_affine is not None:\n",
        "        if np.ndim(target_affine) == 0:\n",
        "            target_affine = np.eye(3) * target_affine\n",
        "        elif np.ndim(target_affine) == 1:\n",
        "            target_affine = np.diag(target_affine)\n",
        "        mask_img = image.resample_img(\n",
        "            mask_img, target_affine=target_affine, interpolation=\"nearest\"\n",
        "        )\n",
        "    masker = maskers.NiftiMasker(mask_img=mask_img).fit()\n",
        "    return masker\n",
        "\n",
        "\n",
        "def coords_to_voxels(coords, ref_img=None):\n",
        "    if ref_img is None:\n",
        "        ref_img = load_mni152_brain_mask()\n",
        "    affine = ref_img.affine\n",
        "    coords = np.atleast_2d(coords)\n",
        "    coords = np.hstack([coords, np.ones((len(coords), 1))])\n",
        "    voxels = np.linalg.pinv(affine).dot(coords.T)[:-1].T\n",
        "    voxels = voxels[(voxels >= 0).all(axis=1)]\n",
        "    voxels = voxels[(voxels < ref_img.shape[:3]).all(axis=1)]\n",
        "    voxels = np.floor(voxels).astype(int)\n",
        "    return voxels\n",
        "\n",
        "\n",
        "def coords_to_peaks_img(coords, mask_img):\n",
        "    mask_img = image.load_img(mask_img)\n",
        "    voxels = coords_to_voxels(coords, mask_img)\n",
        "    peaks = np.zeros(mask_img.shape)\n",
        "    np.add.at(peaks, tuple(voxels.T), 1.0)\n",
        "    peaks_img = image.new_img_like(mask_img, peaks)\n",
        "    return peaks_img\n",
        "\n",
        "\n",
        "def gaussian_coord_smoothing(\n",
        "    coords, mask_img=None, target_affine=None, fwhm=9.0\n",
        "):\n",
        "    masker = get_masker(mask_img, target_affine)\n",
        "    peaks_img = coords_to_peaks_img(coords, mask_img=masker.mask_img_)\n",
        "    img = image.smooth_img(peaks_img, fwhm=fwhm)\n",
        "    return img\n",
        "\n",
        "\n",
        "def coordinates_to_maps(\n",
        "    coordinates, mask_img=None, target_affine=(4, 4, 4), fwhm=9.0\n",
        "):\n",
        "    masker = get_masker(mask_img=mask_img, target_affine=target_affine)\n",
        "    images, img_pmids = [], []\n",
        "    for pmid, img in iter_coordinates_to_maps(\n",
        "        coordinates, mask_img=masker, fwhm=fwhm\n",
        "    ):\n",
        "        images.append(img)\n",
        "        img_pmids.append(pmid)\n",
        "    return images, masker\n",
        "\n",
        "\n",
        "def iter_coordinates_to_maps(\n",
        "    coordinates, mask_img=None, target_affine=(4, 4, 4), fwhm=9.0\n",
        "):\n",
        "    masker = get_masker(mask_img=mask_img, target_affine=target_affine)\n",
        "    articles = coordinates.groupby(\"id\")\n",
        "    for i, (pmid, coord) in enumerate(articles):\n",
        "        img = gaussian_coord_smoothing(\n",
        "            coord.loc[:, [\"x\", \"y\", \"z\"]].values, fwhm=fwhm, mask_img=masker\n",
        "        )\n",
        "        yield pmid, img"
      ],
      "metadata": {
        "id": "rW6ixwgUtQ3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main"
      ],
      "metadata": {
        "id": "HzLlBvXlsIZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    cache_directory = \"/disk1/wyn/workshop/ChatGPT/text2brain-main/cache/\"\n",
        "    out_dir = '/disk1/wyn/workshop/ChatGPT/text2brain-main/data/brain_maps/neuroquery/'\n",
        "\n",
        "    neuroquery_coord = pd.read_table(\n",
        "        '/disk1/wyn/workshop/ChatGPT/text2brain-main/data-neuroquery_version-1_coordinates.tsv.gz') # coordinates data\n",
        "    neuroquery_meta = pd.read_table(\n",
        "        '/disk1/wyn/workshop/ChatGPT/text2brain-main/data-neuroquery_version-1_metadata.tsv.gz') # artilces data\n",
        "    mask_152 = nib.load('/disk1/wyn/workshop/ChatGPT/text2brain-main/mask_img.nii') # brain templates MNI152\n",
        "\n",
        "    coord_to_maps = Memory(cache_directory).cache(coordinates_to_maps)\n",
        "\n",
        "    for index in tqdm(neuroquery_meta.index):\n",
        "        title = neuroquery_meta.at[index, 'title']\n",
        "        id = int(neuroquery_meta.at[index, 'id'])\n",
        "\n",
        "        brain_maps_dir = os.path.join(out_dir, str(id) + '.nii.gz') # save path for every brain map\n",
        "        vol_data = np.zeros((46, 55, 46)) # coordinates space of MNI152 with edges\n",
        "        affine = np.array([\n",
        "            [4., 0., 0., -90.],\n",
        "            [0., 4., 0., -126.],\n",
        "            [0., 0., 4., -72.],\n",
        "            [0., 0., 0., 1.]\n",
        "        ])\n",
        "\n",
        "        coord = neuroquery_coord.loc[neuroquery_coord['id'] == id]\n",
        "        brain_maps, masker = coord_to_maps(\n",
        "            coord, mask_img=mask_152, target_affine=(4, 4, 4), fwhm=9.0\n",
        "        ) # transfer coordinates to maps\n",
        "        brain_map = brain_maps[0]\n",
        "        brain_map = masker.inverse_transform(masker.transform(brain_map).squeeze())\n",
        "        brain_map = brain_map.get_fdata()\n",
        "\n",
        "        vol_data[3:-3, 3:-4, :-6] = brain_map[3:-3, 3:-4, :-6]\n",
        "        brain_img = nib.Nifti1Image(vol_data, affine)\n",
        "\n",
        "        nib.save(brain_img, brain_maps_dir) # save as NibImage"
      ],
      "metadata": {
        "id": "UGFL-ykttjDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatAUG"
      ],
      "metadata": {
        "id": "KmVhTQ0qzX-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFWnJBu_zWXk",
        "outputId": "c257f6f7-eaa3-4162-dc8d-b48ef443e995"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ofO873ZBzcz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "zcUMTHLVzdzH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text2aug"
      ],
      "metadata": {
        "id": "7YcToJ00sNEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text2aug(text):\n",
        "  max_tries = 3\n",
        "  for try_number in range(max_tries):\n",
        "    try:\n",
        "      response = openai.ChatCompletion.create(\n",
        "          model = 'gpt-3.5-turbo',\n",
        "          messages = [\n",
        "              {'role': 'system', 'content': 'You are a brain science expert and I will give you the title of a paper related to brain science next, please rewrite or expand it according to the needs I provide with the given title. Please adhere strictly to the example format.'},\n",
        "              {'role': 'user', 'content': 'Please return two rewrites of the title (One differs more from the original title, one less), one for the abstract expanded by the title (No more than 300 words), one for the experiment you think the subjects underwent according to the title (No more than 100 words), and one for the keywords you think the title covers in brain science.\\nTitle: Neural Patterns of Reorganization after Intensive Robot-Assisted Virtual Reality Therapy and Repetitive Task Practice in Patients with Chronic Stroke'},\n",
        "              {'role': 'assistant', 'content': 'Title1:The Influence of Robot-Assisted Virtual Reality Therapy and Repetitive Task Practice on Neural Reorganization Patterns in Chronic Stroke Patients\\nTitle2:Neural Reorganization Patterns in Chronic Stroke Patients following Intensive Robot-Assisted Virtual Reality Therapy and Repetitive Task Practice\\nAbstract:Several approaches to rehabilitation of the hand following a stroke have emerged over the last two decades. These treatments, including repetitive task practice (RTP), robotically assisted rehabilitation and virtual rehabilitation activities, produce improvements in hand function but have yet to reinstate function to pre-stroke levels-which likely depends on developing the therapies to impact cortical reorganization in a manner that favors or supports recovery. Understanding cortical reorganization that underlies the above interventions is therefore critical to inform how such therapies can be utilized and improved and is the focus of the current investigation. Specifically, we compare neural reorganization elicited in stroke patients participating in two interventions: a hybrid of robot-assisted virtual reality (RAVR) rehabilitation training and a program of RTP training. Ten chronic stroke subjects participated in eight 3-h sessions of RAVR therapy. Another group of nine stroke subjects participated in eight sessions of matched RTP therapy. Functional magnetic resonance imaging (fMRI) data were acquired during paretic hand movement, before and after training. We compared the difference between groups and sessions (before and after training) in terms of BOLD intensity, laterality index of activation in sensorimotor areas, and the effective connectivity between ipsilesional motor cortex (iMC), contralesional motor cortex, ipsilesional primary somatosensory cortex (iS1), ipsilesional ventral premotor area (iPMv), and ipsilesional supplementary motor area. Last, we analyzed the relationship between changes in fMRI data and functional improvement measured by the Jebsen Taylor Hand Function Test (JTHFT), in an attempt to identify how neurophysiological changes are related to motor improvement. Subjects in both groups demonstrated motor recovery after training, but fMRI data revealed RAVR-specific changes in neural reorganization patterns. First, BOLD signal in multiple regions of interest was reduced and re-lateralized to the ipsilesional side. Second, these changes correlated with improvement in JTHFT scores. Our findings suggest that RAVR training may lead to different neurophysiological changes when compared with traditional therapy. This effect may be attributed to the influence that augmented visual and haptic feedback during RAVR training exerts over higher-order somatosensory and visuomotor areas.\\nExperiment:The study involved 19 subjects with chronic ischemic stroke who participated in a 2-week training program, either robotic-assisted virtual rehabilitation (RAVR) or a traditional rehabilitation program (RTP). The RAVR group trained using a virtual reality system with robot assistance, while the RTP group received traditional rehabilitation. Both groups were trained for 4 days a week, 3 hours per day. The study aimed to examine the effectiveness of RAVR in promoting recovery of upper extremity function, as demonstrated by improved Modified Ashworth Scale scores and Chedoke-McMaster Impairment Inventory stages.\\nKeywords:stroke, virtual reality, rehabilitation, motor control and learning/plasticity, functional magnetic resonance imaging neuroimaging, connectivity analysis'},\n",
        "              {'role': 'user', 'content': 'Please return two rewrites of the title (One differs more from the original title, one less), one for the abstract expanded by the title (No more than 300 words), one for the experiment you think the subjects underwent according to the title (No more than 100 words), and one for the keywords you think the title covers in brain science.\\nTitle: {}'.format(text)}\n",
        "          ]\n",
        "      )\n",
        "      return response['choices'][0]['message']['content']\n",
        "\n",
        "    except openai.error.APIError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('APIError')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.Timeout as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('Timeout')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.APIConnectionError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('APIConnectionError')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)"
      ],
      "metadata": {
        "id": "QkY3h14CzfVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main"
      ],
      "metadata": {
        "id": "TR0HAkg1sPW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True: # augment brain datasets\n",
        "  try:\n",
        "    openai.api_key = '[your_key]'\n",
        "\n",
        "    neuroquery_meta = pd.read_table('/content/drive/MyDrive/Colab/CoordinateGPT/data-neuroquery_version-1_metadata.tsv.gz') # artilces data\n",
        "    out_dir = '/content/drive/MyDrive/Colab/CoordinateGPT/ChatAUG/AUG_1'\n",
        "\n",
        "    for index in tqdm(neuroquery_meta.index):\n",
        "      title = neuroquery_meta.at[index, 'title']\n",
        "      id = int(neuroquery_meta.at[index, 'id'])\n",
        "\n",
        "      save_dir = os.path.join(out_dir, str(id) + '.npy')\n",
        "      if os.path.exists(save_dir):\n",
        "        continue\n",
        "\n",
        "      text = tuple([title])\n",
        "      text_aug = text2aug(text) # augmentation of five types of data\n",
        "\n",
        "      A = text_aug.split('Title1:')\n",
        "      B = A[1].split('Title2:')\n",
        "      C = B[1].split('Abstract:')\n",
        "      D = C[1].split('Experiment:')\n",
        "      E = D[1].split('Keywords:')\n",
        "\n",
        "      Title1 = B[0][:-1]\n",
        "      Title2 = C[0][:-1]\n",
        "      Abstract = D[0][:-1]\n",
        "      Experiment = E[0][:-1]\n",
        "      Keyword = E[1]\n",
        "\n",
        "      aug = np.array({'Title1': Title1, 'Title2': Title2, 'Abstract': Abstract, 'Experiment': Experiment, 'Keyword': Keyword})\n",
        "      np.save(save_dir, aug)\n",
        "\n",
        "      time.sleep(1.25)\n",
        "\n",
        "\n",
        "    break # end loops\n",
        "\n",
        "  except Exception as e: # avoid aborting chatgpt when it encounters an error\n",
        "    time.sleep(5)\n",
        "    continue"
      ],
      "metadata": {
        "id": "IizTCxrBziFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat2Brain_Train"
      ],
      "metadata": {
        "id": "p1oPsvAn052o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "adlNUg712JFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MdLPaObr2TKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from argparse import ArgumentParser"
      ],
      "metadata": {
        "id": "ZWq2mrmC05Kg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_decoder"
      ],
      "metadata": {
        "id": "-Y3aS4s_2s5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "\n",
        "    def forward(self, input_):\n",
        "        out = self.conv1(input_)\n",
        "        out = self.bn1(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        identity = self.conv1(input_)\n",
        "        residue = self.bn1(identity)\n",
        "        residue = self.act_fn(residue)\n",
        "        residue = self.conv2(residue)\n",
        "        out = identity + residue\n",
        "        out = self.bn2(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.trans_conv1 = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, output_padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "\n",
        "    def forward(self, input_):\n",
        "        out = self.trans_conv1(input_)\n",
        "        out = self.bn1(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImageDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn=nn.Sigmoid, num_filter=256):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_filter = num_filter\n",
        "        act_fn = nn.Hardtanh(min_val=-6, max_val=6)\n",
        "\n",
        "        self.trans_1 = TransConvResBlock3D(self.in_channels, self.num_filter, act_fn)\n",
        "        self.trans_2 = TransConvResBlock3D(self.num_filter, self.num_filter // 2, act_fn)\n",
        "        self.trans_3 = TransConvResBlock3D(self.num_filter // 2, self.num_filter // 4, act_fn)\n",
        "\n",
        "        self.out = SimpleConvResBlock3D(self.num_filter // 4, self.out_channels, act_fn)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        up_1 = self.trans_1(input_)\n",
        "        up_2 = self.trans_2(up_1)\n",
        "        up_3 = self.trans_3(up_2)\n",
        "\n",
        "        out = self.out(up_3)\n",
        "\n",
        "        return out[:, :, 1:, 1:, 1:]"
      ],
      "metadata": {
        "id": "oN17fa1k2rUT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_main_with_encoder"
      ],
      "metadata": {
        "id": "gn16AQT626Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Text2BrainModel(nn.Module):\n",
        "    def __init__(self, out_channels, fc_channels, decoder_filters, pretrained_bert_dir, decoder_act_fn=nn.Sigmoid, drop_p=0.5, decoder_input_shape=[4, 5, 4]):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.fc_channels = fc_channels\n",
        "        self.decoder_filters = decoder_filters\n",
        "        self.decoder_input_shape = decoder_input_shape\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(pretrained_bert_dir)\n",
        "        self.encoder = transformers.BertModel.from_pretrained(pretrained_bert_dir)\n",
        "        if torch.cuda.is_available():\n",
        "          self.encoder = self.encoder.cuda()\n",
        "\n",
        "        self.fc = nn.Linear(\n",
        "          in_features=768,\n",
        "          out_features=self.decoder_input_shape[0]*self.decoder_input_shape[1]*self.decoder_input_shape[2]*self.fc_channels)\n",
        "        self.dropout = nn.Dropout(self.drop_p)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.decoder = ImageDecoder(in_channels=self.fc_channels, out_channels=1, num_filter=self.decoder_filters, act_fn=decoder_act_fn)\n",
        "\n",
        "\n",
        "    def forward(self, texts):\n",
        "        batch = [self._tokenize(x) for x in texts]\n",
        "\n",
        "        in_mask = self._pad_mask(batch, batch_first=True)\n",
        "        in_ = pad_sequence(batch, batch_first=True)\n",
        "        if torch.cuda.is_available():\n",
        "          in_ = in_.cuda()\n",
        "          in_mask = in_mask.cuda()\n",
        "\n",
        "        _, embedding = self.encoder(in_, attention_mask=in_mask)\n",
        "\n",
        "        x = self.dropout(embedding)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        decoder_tensor_shape = [-1, self.fc_channels] + self.decoder_input_shape\n",
        "        x = x.view(decoder_tensor_shape)\n",
        "\n",
        "        out = self.decoder(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return self.tokenizer.encode(text, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=512).squeeze(0)\n",
        "\n",
        "\n",
        "    def _pad_mask(self, sequences, batch_first=False):\n",
        "        ret = [torch.ones(len(s)) for s in sequences]\n",
        "        return pad_sequence(ret, batch_first=batch_first)"
      ],
      "metadata": {
        "id": "gfN6_lDJ21q4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "id": "wUMGFHJ43I47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Chat2BrainDataset(Dataset):\n",
        "    def __init__(self, metadata, text_dir, brain_dir, source):\n",
        "        self.metadata = metadata\n",
        "        self.text_dir = text_dir\n",
        "        self.brain_dir = brain_dir\n",
        "        self.source = source\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.metadata.iloc[index]\n",
        "        id = row['id']\n",
        "\n",
        "        if self.source == 'title':\n",
        "            text = row['title']\n",
        "        elif self.source == 'aug':\n",
        "            Aug = np.load(os.path.join(self.text_dir, str(id) + '.npy'), allow_pickle=True)\n",
        "            Title = row['title']\n",
        "\n",
        "            text = [Title, Aug]\n",
        "        else:\n",
        "            raise Exception(\"Data source not implemented\")\n",
        "        text = text.lower()\n",
        "\n",
        "        brain_map = nib.load(os.path.join(self.brain_dir, str(id) + '.nii.gz'))\n",
        "        brain_map = brain_map.get_fdata()\n",
        "        brain_map = brain_map[3:-3, 3:-4, :-6]\n",
        "        brain_map = np.expand_dims(brain_map, 0)\n",
        "        brain_map = brain_map / np.max(brain_map)\n",
        "        brain_map = np.nan_to_num(brain_map, copy=False)\n",
        "\n",
        "        return text, torch.cuda.FloatTensor(brain_map)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata.index)"
      ],
      "metadata": {
        "id": "2FjLHLwe3D0I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils"
      ],
      "metadata": {
        "id": "E-NeKvI54HMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_corr_coeff(A, B):\n",
        "    # rowwise mean of input arrays & subtract from input arrays\n",
        "    A_mA = A - A.mean(1)[:, None]\n",
        "    B_mB = B - B.mean(1)[:, None]\n",
        "\n",
        "    # sum of squares across rows\n",
        "    ssA = (A_mA**2).sum(1)\n",
        "    ssB = (B_mB**2).sum(1)\n",
        "\n",
        "    # corr coeff\n",
        "    return np.dot(A_mA, B_mB.T) / np.sqrt(np.dot(ssA[:, None], ssB[None]))\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, fname, output_dir):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, os.path.join(output_dir, fname))"
      ],
      "metadata": {
        "id": "1Mz94wlC35aV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "args"
      ],
      "metadata": {
        "id": "7bp-TEPF4lnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_args():\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--gpus\", type=str,\n",
        "                        default=\"0, 1, 2, 3\",\n",
        "                        help=\"Which gpus to use?\")\n",
        "\n",
        "    parser.add_argument(\"--ver\", type=str,\n",
        "                        default=\"neuroquery\",\n",
        "                        help=\"Additional string for the name of the file\")\n",
        "\n",
        "    parser.add_argument(\"--train_csv\",\n",
        "                        type=str,\n",
        "                        help=\"Path to the csv containing the training articles data\")\n",
        "\n",
        "    parser.add_argument(\"--val_csv\",\n",
        "                        type=str,\n",
        "                        help=\"Path to the csv containing the validation articles data\")\n",
        "\n",
        "    parser.add_argument(\"--images_dir\",\n",
        "                        type=str,\n",
        "                        help=\"Directory containing activation maps, should be of size (40, 48, 40)\")\n",
        "\n",
        "    parser.add_argument(\"--pretrained_bert_dir\",\n",
        "                        type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/scibert_scivocab_uncased\",\n",
        "                        help=\"Directory containing pretrained BERT model\")\n",
        "\n",
        "    parser.add_argument(\"--pretrained_tokenizer_dir\",\n",
        "                        type=str,\n",
        "                        help=\"Directory containing pretrained tokenizer\")\n",
        "\n",
        "    parser.add_argument(\"--mask_file\",\n",
        "                        type=str,\n",
        "                        help=\"Brain mask file\")\n",
        "\n",
        "    parser.add_argument(\"--save_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/Chat2Brain_checkpoint/\",\n",
        "                        help=\"Path to the output directory\")\n",
        "\n",
        "    parser.add_argument(\"--save_test_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/Chat2Brain_test/\",\n",
        "                        help=\"Path to the output directory\")\n",
        "\n",
        "    parser.add_argument(\"--mask_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/data/brain_maps/neuroquery/\",\n",
        "                        help=\"Path to the mask directory\")\n",
        "\n",
        "    parser.add_argument(\"--text_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/data/ChatAUG/\",\n",
        "                        help=\"Path to the text directory\")\n",
        "\n",
        "    parser.add_argument(\"--metadata_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/data-neuroquery_version-1_metadata.tsv.gz\",\n",
        "                        help=\"Path to the metadata directory\")\n",
        "\n",
        "    parser.add_argument(\"--n_fc_channels\",\n",
        "                        type=int,\n",
        "                        default=1024,\n",
        "                        help=\"Base number of channels in the FC layer\")\n",
        "\n",
        "    parser.add_argument(\"--n_decoder_channels\",\n",
        "                        type=int,\n",
        "                        default=256,\n",
        "                        help=\"Base number of channels in the image decoder\")\n",
        "\n",
        "    parser.add_argument(\"--n_output_channels\",\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help=\"Number of output channels\")\n",
        "\n",
        "    parser.add_argument(\"--lr\",\n",
        "                        type=float,\n",
        "                        default=3e-2,\n",
        "                        help=\"Learning rate\")\n",
        "\n",
        "    parser.add_argument(\"--weight_decay\",\n",
        "                        type=float,\n",
        "                        default=1e-6,\n",
        "                        help=\"Weight decay of the optimizer\")\n",
        "\n",
        "    parser.add_argument(\"--drop_p\",\n",
        "                        type=float,\n",
        "                        default=0.6,\n",
        "                        help=\"Dropout proportion for FC layer\")\n",
        "\n",
        "    parser.add_argument(\"--epochs\",\n",
        "                        type=int,\n",
        "                        default=550,\n",
        "                        help=\"Training epochs\")\n",
        "\n",
        "    parser.add_argument(\"--seed\",\n",
        "                        type=int,\n",
        "                        default=28)\n",
        "\n",
        "    parser.add_argument(\"--random_seed\",\n",
        "                        type=int,\n",
        "                        default=60)\n",
        "\n",
        "    parser.add_argument(\"--split\",\n",
        "                        type=list,\n",
        "                        default=[6, 2, 2])\n",
        "\n",
        "    parser.add_argument(\"--checkpoint_file\",\n",
        "                        type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/Chat2Brain_checkpoint/neuroquery_title_fc1024_dec256_lr0.03_decay1e-06_drop0.6_seed28/checkpoint_1450.pth\",\n",
        "                        help=\"Path to the checkpoint file to be loaded into the model\")\n",
        "\n",
        "    parser.add_argument(\"--checkpoint_interval\",\n",
        "                        type=int,\n",
        "                        default=10,\n",
        "                        help=\"Number of epochs between saved checkpoints\")\n",
        "\n",
        "    parser.add_argument(\"--batch_size\",\n",
        "                        type=int,\n",
        "                        default=24,\n",
        "                        help=\"Batch size\")\n",
        "\n",
        "    parser.add_argument(\"--Scaling_factor\",\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help=\"Scaling factor\")\n",
        "\n",
        "    parser.add_argument(\"--phrase\",\n",
        "                        type=str,\n",
        "                        default=None,\n",
        "                        help=\"Input phrase for prediction\")\n",
        "\n",
        "    parser.add_argument(\"--source\",\n",
        "                        type=str,\n",
        "                        default=\"title\",\n",
        "                        help=\"Source type\")\n",
        "\n",
        "    return parser.parse_args()"
      ],
      "metadata": {
        "id": "Nepo0--u4jYa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "uxkvg5vZ4vjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, loss_fn, mask):\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    avg_corr = 0\n",
        "\n",
        "    for batch_idx, (text, brain) in enumerate(train_loader):\n",
        "        brain_map = (args.Scaling_factor * brain).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if args.source == 'title':\n",
        "            output = model(text)\n",
        "\n",
        "            loss = loss_fn(output, brain_map)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += (loss.item() / len(train_loader))\n",
        "        elif args.source == 'aug':\n",
        "            Title = text[0]\n",
        "            Title1 = text[1]['Title1']\n",
        "            Title2 = text[1]['Title2']\n",
        "            Abstract = text[1]['Abstract']\n",
        "            Experiment = text[1]['Experiment']\n",
        "            Keywords = text[1]['Keywords']\n",
        "\n",
        "            for text_with_aug in [Title, Title1, Title2, Abstract, Experiment, Keywords, Title]:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                output = model(text_with_aug)\n",
        "\n",
        "                loss = loss_fn(output, brain_map)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                avg_loss += (loss.item() / len(train_loader * 7))\n",
        "\n",
        "        output_np = output.cpu().detach().numpy()\n",
        "        target_np = brain_map.cpu().detach().numpy()\n",
        "\n",
        "        all_corr = compute_corr_coeff(\n",
        "            output_np.reshape(output_np.shape[0], -1),\n",
        "            target_np.reshape(output_np.shape[0], -1))\n",
        "        corr = np.mean(np.diag(all_corr))\n",
        "        if np.isnan(corr):\n",
        "            print(text,\n",
        "                  np.isnan(output_np).any(),\n",
        "                  np.isnan(target_np).any(), all_corr, corr)\n",
        "            print(\"Output\", torch.max(output), output)\n",
        "            print(\"Target\", torch.max(brain_map), brain_map)\n",
        "            sys.exit(1)\n",
        "\n",
        "        avg_corr = avg_corr + corr / len(train_loader)\n",
        "\n",
        "        if batch_idx % 100 == 99:\n",
        "            print('[{}/{} ({:.0f}%)] Loss: {:.6f} Corr: {:.6f}'.format(\n",
        "                batch_idx * len(text), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), corr))\n",
        "\n",
        "    print('  Train: avg loss: {:.6f} - avg corr: {:.6f}'.format(avg_loss, avg_corr))\n",
        "\n",
        "    return avg_loss, avg_corr"
      ],
      "metadata": {
        "id": "U5143teu4xCG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "eval"
      ],
      "metadata": {
        "id": "WOMytl_a5Ggk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, val_loader, loss_fn, mask):\n",
        "    model.eval()\n",
        "    avg_loss = 0\n",
        "    avg_corr = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (text, brain) in enumerate(val_loader):\n",
        "            brain_map = (args.Scaling_factor * brain).cuda()\n",
        "\n",
        "            if args.source == 'title':\n",
        "                output = model(text)\n",
        "\n",
        "                loss = loss_fn(output, brain_map)\n",
        "                avg_loss += (loss.item() / len(val_loader))\n",
        "            elif args.source == 'aug':\n",
        "                Title = text[0]\n",
        "                Title1 = text[1]['Title1']\n",
        "                Title2 = text[1]['Title2']\n",
        "                Abstract = text[1]['Abstract']\n",
        "                Experiment = text[1]['Experiment']\n",
        "                Keywords = text[1]['Keywords']\n",
        "\n",
        "                for text_with_aug in [Title, Title1, Title2, Abstract, Experiment, Keywords, Title]:\n",
        "\n",
        "                    output = model(text_with_aug)\n",
        "\n",
        "                    loss = loss_fn(output, brain_map)\n",
        "                    avg_loss += (loss.item() / len(val_loader * 7))\n",
        "\n",
        "            output_np = output.cpu().detach().numpy()[:, :, mask]\n",
        "            target_np = brain_map.cpu().detach().numpy()[:, :, mask]\n",
        "            corr = np.mean(\n",
        "                np.diag(\n",
        "                    compute_corr_coeff(\n",
        "                        output_np.reshape(output_np.shape[0], -1),\n",
        "                        target_np.reshape(output_np.shape[0], -1))))\n",
        "            avg_corr = avg_corr + corr / len(val_loader)\n",
        "\n",
        "        print('  Val: avg loss: {:.6f} - avg corr: {:.6f}'.format(\n",
        "            avg_loss, avg_corr))\n",
        "\n",
        "        return avg_loss, avg_corr"
      ],
      "metadata": {
        "id": "q7ia49O15Hpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main"
      ],
      "metadata": {
        "id": "90bYNej35fIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    # init\n",
        "    args = init_args()\n",
        "\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
        "\n",
        "    output_name = f'continue_{args.ver}_{args.source}_fc{args.n_fc_channels}_dec{args.n_decoder_channels}_lr{args.lr}_decay{args.weight_decay}_drop{args.drop_p}_seed{args.seed}'\n",
        "    output_dir = os.path.join(args.save_dir, output_name)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    else:\n",
        "        print(f'Output dir exists: {output_dir}')\n",
        "\n",
        "    mask_dir = args.mask_dir\n",
        "\n",
        "    writer = SummaryWriter(os.path.join(output_dir, \"logs\")) # tensorboard\n",
        "\n",
        "    # load Data\n",
        "    np.random.seed(args.seed)\n",
        "    meta_data = pd.read_table(args.metadata_dir)\n",
        "\n",
        "    # randomly divide the dataset into training, validation and test sets\n",
        "    train_meta = meta_data.sample(frac=args.split[0] / (args.split[0] + args.split[1] + args.split[2]), random_state=args.random_seed)\n",
        "\n",
        "    between_meta = meta_data.append(train_meta).drop_duplicates(keep=False)\n",
        "\n",
        "    val_meta = between_meta.sample(frac=args.split[1] / (args.split[1] + args.split[2]), random_state=args.random_seed)\n",
        "    test_meta = between_meta.append(val_meta).drop_duplicates(keep=False)\n",
        "\n",
        "    train_meta.to_csv(os.path.join(args.save_dir, 'train_meta.csv'))\n",
        "    val_meta.to_csv(os.path.join(args.save_dir, 'val_meta.csv'))\n",
        "    test_meta.to_csv(os.path.join(args.save_dir, 'test_meta.csv'))\n",
        "\n",
        "    train_dataset = Chat2BrainDataset(train_meta, args.text_dir, args.mask_dir, args.source)\n",
        "    val_dataset = Chat2BrainDataset(val_meta, args.text_dir, args.mask_dir, args.source)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "    print(\"Number of training articles:\", len(train_dataset))\n",
        "    print(\"Number of validation articles:\", len(val_dataset))\n",
        "\n",
        "    # init model\n",
        "    model = Text2BrainModel(\n",
        "        out_channels=1,\n",
        "        fc_channels=args.n_fc_channels,\n",
        "        decoder_filters=args.n_decoder_channels,\n",
        "        pretrained_bert_dir=args.pretrained_bert_dir,\n",
        "        drop_p=args.drop_p)\n",
        "    model.cuda()\n",
        "\n",
        "    # loading checkpoint\n",
        "    if args.checkpoint_file is not None:\n",
        "        state_dict = torch.load(args.checkpoint_file)['state_dict']\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    # optimizer\n",
        "    num_training_steps = len(train_dataset) * args.epochs\n",
        "    num_warmup_steps = num_training_steps // 3\n",
        "    opt = transformers.AdamW([\n",
        "        {'params': model.fc.parameters()},\n",
        "        {'params': model.decoder.parameters()},\n",
        "        {'params': model.encoder.parameters(), 'lr': 1e-5},\n",
        "    ], lr=args.lr, weight_decay=args.weight_decay)\n",
        "    sched = transformers.get_linear_schedule_with_warmup(opt, num_warmup_steps, num_training_steps)\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "    val_losses = []\n",
        "    val_corrs = []\n",
        "    best_loss = sys.float_info.max\n",
        "    best_corr = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(args.epochs)):\n",
        "        train_loss, train_corr = train(model, train_loader, opt, loss_fn, [2, 4])\n",
        "        val_loss, val_corr = eval(model, val_loader, loss_fn, [2, 4])\n",
        "\n",
        "        writer.add_scalar('training loss', train_loss, epoch)\n",
        "        writer.add_scalar('training corr', train_corr, epoch)\n",
        "\n",
        "        writer.add_scalar('validation loss', val_loss, epoch)\n",
        "        writer.add_scalar('validation corr', val_corr, epoch)\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_corrs.append(val_corr)\n",
        "\n",
        "        mean_loss = np.mean(val_losses[-args.checkpoint_interval:])\n",
        "        mean_corr = np.mean(val_corrs[-args.checkpoint_interval:])\n",
        "\n",
        "        if (epoch > args.epochs * 0.1) and (epoch % args.checkpoint_interval == 0):\n",
        "            if mean_loss < best_loss:\n",
        "                save_checkpoint(model, opt, sched, epoch, \"best_loss.pth\", output_dir)\n",
        "                best_loss = mean_loss\n",
        "            if mean_corr > best_corr:\n",
        "                save_checkpoint(model, opt, sched, epoch, \"best_corr.pth\", output_dir)\n",
        "                best_corr = mean_corr\n",
        "            save_checkpoint(model, opt, sched, epoch, f'checkpoint_{epoch}.pth', output_dir)\n",
        "        sched.step()\n",
        "    save_checkpoint(model, opt, sched, args.epochs, f'checkpoint_{args.epochs}.pth', output_dir)\n",
        "    writer.close()"
      ],
      "metadata": {
        "id": "VkVchXGm5XNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat2Brain_Train_DDP"
      ],
      "metadata": {
        "id": "S7BjDgSP5iLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "iv2RGeW96u9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HppdYzJd6wTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import argparse\n",
        "import torch\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from argparse import ArgumentParser"
      ],
      "metadata": {
        "id": "F5IN3iQV6CPK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_decoder"
      ],
      "metadata": {
        "id": "_EzbkEDh63yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "\n",
        "    def forward(self, input_):\n",
        "        out = self.conv1(input_)\n",
        "        out = self.bn1(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        identity = self.conv1(input_)\n",
        "        residue = self.bn1(identity)\n",
        "        residue = self.act_fn(residue)\n",
        "        residue = self.conv2(residue)\n",
        "        out = identity + residue\n",
        "        out = self.bn2(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.trans_conv1 = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, output_padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "\n",
        "    def forward(self, input_):\n",
        "        out = self.trans_conv1(input_)\n",
        "        out = self.bn1(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImageDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn=nn.Sigmoid, num_filter=256):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_filter = num_filter\n",
        "        act_fn = nn.Hardtanh(min_val=-6, max_val=6)\n",
        "\n",
        "        self.trans_1 = TransConvResBlock3D(self.in_channels, self.num_filter, act_fn)\n",
        "        self.trans_2 = TransConvResBlock3D(self.num_filter, self.num_filter // 2, act_fn)\n",
        "        self.trans_3 = TransConvResBlock3D(self.num_filter // 2, self.num_filter // 4, act_fn)\n",
        "\n",
        "        self.out = SimpleConvResBlock3D(self.num_filter // 4, self.out_channels, act_fn)\n",
        "\n",
        "\n",
        "    def forward(self, input_):\n",
        "        up_1 = self.trans_1(input_)\n",
        "        up_2 = self.trans_2(up_1)\n",
        "        up_3 = self.trans_3(up_2)\n",
        "\n",
        "        out = self.out(up_3)\n",
        "\n",
        "        return out[:, :, 1:, 1:, 1:]"
      ],
      "metadata": {
        "id": "BS_IhHX362o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_main_with_encoder"
      ],
      "metadata": {
        "id": "IqT8VI4H69Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Text2BrainModel(nn.Module):\n",
        "    def __init__(self, out_channels, fc_channels, decoder_filters, pretrained_bert_dir, decoder_act_fn=nn.Sigmoid, drop_p=0.5, decoder_input_shape=[4, 5, 4]):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.fc_channels = fc_channels\n",
        "        self.decoder_filters = decoder_filters\n",
        "        self.decoder_input_shape = decoder_input_shape\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(pretrained_bert_dir)\n",
        "        self.encoder = transformers.BertModel.from_pretrained(pretrained_bert_dir)\n",
        "        if torch.cuda.is_available():\n",
        "          self.encoder = self.encoder.cuda()\n",
        "\n",
        "        self.fc = nn.Linear(\n",
        "          in_features=768,\n",
        "          out_features=self.decoder_input_shape[0]*self.decoder_input_shape[1]*self.decoder_input_shape[2]*self.fc_channels)\n",
        "        self.dropout = nn.Dropout(self.drop_p)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.decoder = ImageDecoder(in_channels=self.fc_channels, out_channels=1, num_filter=self.decoder_filters, act_fn=decoder_act_fn)\n",
        "\n",
        "\n",
        "    def forward(self, texts):\n",
        "        batch = [self._tokenize(x) for x in texts]\n",
        "\n",
        "        in_mask = self._pad_mask(batch, batch_first=True)\n",
        "        in_ = pad_sequence(batch, batch_first=True)\n",
        "        if torch.cuda.is_available():\n",
        "          in_ = in_.cuda()\n",
        "          in_mask = in_mask.cuda()\n",
        "\n",
        "        _, embedding = self.encoder(in_, attention_mask=in_mask)\n",
        "\n",
        "        x = self.dropout(embedding)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        decoder_tensor_shape = [-1, self.fc_channels] + self.decoder_input_shape\n",
        "        x = x.view(decoder_tensor_shape)\n",
        "\n",
        "        out = self.decoder(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return self.tokenizer.encode(text, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=512).squeeze(0)\n",
        "\n",
        "\n",
        "    def _pad_mask(self, sequences, batch_first=False):\n",
        "        ret = [torch.ones(len(s)) for s in sequences]\n",
        "        return pad_sequence(ret, batch_first=batch_first)"
      ],
      "metadata": {
        "id": "LmWiyRpy68XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "id": "jvBIuOX958RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Chat2BrainDataset(Dataset):\n",
        "    def __init__(self, metadata, text_dir, brain_dir, source):\n",
        "        self.metadata = metadata\n",
        "        self.text_dir = text_dir\n",
        "        self.brain_dir = brain_dir\n",
        "        self.source = source\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.metadata.iloc[index]\n",
        "        id = row['id']\n",
        "\n",
        "        if self.source == 'title':\n",
        "            text = row['title']\n",
        "            text = text.lower()\n",
        "        elif self.source == 'aug':\n",
        "            Aug = np.load(os.path.join(self.text_dir, str(id) + '.npy'), allow_pickle=True)\n",
        "            Title = row['title']\n",
        "\n",
        "            text = Aug.tolist()\n",
        "            text['Title'] = Title.lower()\n",
        "            text['Title1'] = text['Title1'].lower()\n",
        "            text['Title2'] = text['Title2'].lower()\n",
        "            text['Abstract'] = text['Abstract'].lower()\n",
        "            text['Experiment'] = text['Experiment'].lower()\n",
        "            text['Keyword'] = text['Keyword'].lower()\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"Data source not implemented\")\n",
        "\n",
        "        brain_map = nib.load(os.path.join(self.brain_dir, str(id) + '.nii.gz'))\n",
        "        brain_map = brain_map.get_fdata()\n",
        "        brain_map = brain_map[3:-3, 3:-4, :-6]\n",
        "        brain_map = np.expand_dims(brain_map, 0)\n",
        "        brain_map = brain_map / np.max(brain_map)\n",
        "        brain_map = np.nan_to_num(brain_map, copy=False)\n",
        "        brain_map = brain_map.astype(np.float32)\n",
        "        return text, brain_map\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata.index)"
      ],
      "metadata": {
        "id": "3VOtItMb5kYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils"
      ],
      "metadata": {
        "id": "F8Y1iz3Q7DrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_corr_coeff(A, B):\n",
        "    # rowwise mean of input arrays & subtract from input arrays\n",
        "    A_mA = A - A.mean(1)[:, None]\n",
        "    B_mB = B - B.mean(1)[:, None]\n",
        "\n",
        "    # sum of squares across rows\n",
        "    ssA = (A_mA**2).sum(1)\n",
        "    ssB = (B_mB**2).sum(1)\n",
        "\n",
        "    # corr coeff\n",
        "    return np.dot(A_mA, B_mB.T) / np.sqrt(np.dot(ssA[:, None], ssB[None]))\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, fname, output_dir):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'scheduler': scheduler.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, os.path.join(output_dir, fname))"
      ],
      "metadata": {
        "id": "KSJt2R307DA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils_ddp"
      ],
      "metadata": {
        "id": "JhxFzZuE7FDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mean(tensor, nprocs):\n",
        "    # average loss on different threads\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n",
        "    rt /= nprocs\n",
        "    return rt"
      ],
      "metadata": {
        "id": "qYgNt3XG6l_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "IrYBQ7X47Knh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(local_rank, model, train_loader, optimizer, loss_fn, source):\n",
        "    model.train()\n",
        "    avg_loss = 0\n",
        "    avg_corr = 0\n",
        "\n",
        "    for batch_idx, (text, brain) in enumerate(train_loader):\n",
        "        brain_map = torch.FloatTensor(1 * brain).cuda(local_rank)\n",
        "\n",
        "        if source == 'title':\n",
        "            optimizer.zero_grad()\n",
        "            output = model(text)\n",
        "\n",
        "            loss = loss_fn(output, brain_map)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss = reduce_mean(loss, dist.get_world_size())\n",
        "            avg_loss += (loss.item() / (len(train_loader) * 7))\n",
        "        elif source == 'aug':\n",
        "            Title = text['Title']\n",
        "\n",
        "            Title1 = text['Title1']\n",
        "            Title2 = text['Title2']\n",
        "            Abstract = text['Abstract']\n",
        "            Experiment = text['Experiment']\n",
        "            Keyword = text['Keyword']\n",
        "\n",
        "            for text_with_aug in [Title, Title1, Title2, Abstract, Experiment, Keyword, Title]:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                output = model(text_with_aug)\n",
        "\n",
        "                loss = loss_fn(output, brain_map)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss = reduce_mean(loss, dist.get_world_size())\n",
        "                avg_loss += (loss.item() / (len(train_loader) * 7))\n",
        "\n",
        "        output_np = output.cpu().detach().numpy()\n",
        "        target_np = brain_map.cpu().detach().numpy()\n",
        "\n",
        "        all_corr = compute_corr_coeff(\n",
        "            output_np.reshape(output_np.shape[0], -1),\n",
        "            target_np.reshape(output_np.shape[0], -1))\n",
        "        corr = np.mean(np.diag(all_corr))\n",
        "        if np.isnan(corr):\n",
        "            print(text,\n",
        "                  np.isnan(output_np).any(),\n",
        "                  np.isnan(target_np).any(), all_corr, corr)\n",
        "            print(\"Output\", torch.max(output), output)\n",
        "            print(\"Target\", torch.max(brain_map), brain_map)\n",
        "            sys.exit(1)\n",
        "\n",
        "        avg_corr = avg_corr + corr / (len(train_loader) * 7)\n",
        "\n",
        "        if batch_idx % 100 == 99:\n",
        "            print('[{}/{} ({:.0f}%)] Loss: {:.6f} Corr: {:.6f}'.format(\n",
        "                batch_idx * len(text), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), corr))\n",
        "\n",
        "    print('  Train: avg loss: {:.6f} - avg corr: {:.6f}'.format(avg_loss, avg_corr))\n",
        "\n",
        "    return avg_loss, avg_corr"
      ],
      "metadata": {
        "id": "ur2Q7Zej7Lco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "eval"
      ],
      "metadata": {
        "id": "MmXedvda7Qjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(local_rank, model, val_loader, loss_fn, source):\n",
        "    model.eval()\n",
        "    avg_loss = 0\n",
        "    avg_corr = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (text, brain) in enumerate(val_loader):\n",
        "            brain_map = torch.FloatTensor(1 * brain).cuda(local_rank)\n",
        "\n",
        "            if source == 'title':\n",
        "                output = model(text)\n",
        "\n",
        "                loss = loss_fn(output, brain_map)\n",
        "                loss = reduce_mean(loss, dist.get_world_size())\n",
        "                avg_loss += (loss.item() / (len(val_loader) * 7))\n",
        "            elif source == 'aug':\n",
        "                Title = text['Title']\n",
        "\n",
        "                Title1 = text['Title1']\n",
        "                Title2 = text['Title2']\n",
        "                Abstract = text['Abstract']\n",
        "                Experiment = text['Experiment']\n",
        "                Keyword = text['Keyword']\n",
        "\n",
        "                for text_with_aug in [Title, Title1, Title2, Abstract, Experiment, Keyword, Title]:\n",
        "\n",
        "                    output = model(text_with_aug)\n",
        "\n",
        "                    loss = loss_fn(output, brain_map)\n",
        "                    loss = reduce_mean(loss, dist.get_world_size())\n",
        "                    avg_loss += (loss.item() / (len(val_loader) * 7))\n",
        "\n",
        "            output_np = output.cpu().detach().numpy()\n",
        "            target_np = brain_map.cpu().detach().numpy()\n",
        "            corr = np.mean(\n",
        "                np.diag(\n",
        "                    compute_corr_coeff(\n",
        "                        output_np.reshape(output_np.shape[0], -1),\n",
        "                        target_np.reshape(output_np.shape[0], -1))))\n",
        "            avg_corr = avg_corr + corr / (len(val_loader) * 7)\n",
        "\n",
        "        print('  Val: avg loss: {:.6f} - avg corr: {:.6f}'.format(\n",
        "            avg_loss, avg_corr))\n",
        "\n",
        "        return avg_loss, avg_corr"
      ],
      "metadata": {
        "id": "FoOdTizj7P60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main"
      ],
      "metadata": {
        "id": "kI5eFEGU7mo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "python -m torch.distributed.launch --nproc_per_node=**[num_gpu(int)]** **[python script]**\n",
        "\n",
        "eg:\n",
        "\n",
        "python -m torch.distributed.launch --nproc_per_node=**8 Chat2Brain_train.py**"
      ],
      "metadata": {
        "id": "qdH2ftSOmlJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    # init, can't load arguments with args when using DDP\n",
        "    # args = init_args()\n",
        "\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3, 4, 5, 6, 7'\n",
        "\n",
        "    # init ddp\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--local_rank', type=int)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    dist.init_process_group(backend='nccl', world_size=8, rank=args.local_rank) # nccl is the best backend, world_size should be equal to num_gpu\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "\n",
        "    batch_size = 48 // dist.get_world_size() # total batchsize is the sum of each gpu's batchsize\n",
        "\n",
        "    output_name = f'neuroquery_aug_fc1024_dec256_lr3e-2_decay1e-6_drop0.6_seed28'\n",
        "    output_dir = os.path.join(\"/data3/weiyaonai/project/chat2brain/chat2brain/Chat2Brain_checkpoint/\", output_name)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    else:\n",
        "        if dist.get_rank() == 0:\n",
        "            print(f'Output dir exists: {output_dir}')\n",
        "\n",
        "    mask_dir = \"/data3/weiyaonai/project/chat2brain/chat2brain/data/brain_maps/neuroquery/\"\n",
        "\n",
        "    if dist.get_rank() == 0:\n",
        "        writer = SummaryWriter(os.path.join(output_dir, \"logs\")) # tensorboard\n",
        "\n",
        "    # load Data\n",
        "    np.random.seed(28)\n",
        "    meta_data = pd.read_table(\"/data3/weiyaonai/project/chat2brain/chat2brain/data-neuroquery_version-1_metadata.tsv.gz\")\n",
        "\n",
        "    # randomly divide the dataset into training, validation and test sets\n",
        "    train_meta = meta_data.sample(frac=6 / (6 + 2 + 2), random_state=60)\n",
        "\n",
        "    between_meta = meta_data.append(train_meta).drop_duplicates(keep=False)\n",
        "\n",
        "    val_meta = between_meta.sample(frac=2 / (2 + 2), random_state=60)\n",
        "\n",
        "    train_dataset = Chat2BrainDataset(train_meta, \"/data3/weiyaonai/project/chat2brain/chat2brain/data/ChatAUG/AUG_1/\", \"/data3/weiyaonai/project/chat2brain/chat2brain/data/brain_maps/neuroquery/\", \"aug\")\n",
        "    val_dataset = Chat2BrainDataset(val_meta, \"/data3/weiyaonai/project/chat2brain/chat2brain/data/ChatAUG/AUG_1/\", \"/data3/weiyaonai/project/chat2brain/chat2brain/data/brain_maps/neuroquery/\", \"aug\")\n",
        "\n",
        "    # data parallelism\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "    val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "    if dist.get_rank() == 0:\n",
        "        print(\"Number of training articles:\", len(train_dataset))\n",
        "        print(\"Number of validation articles:\", len(val_dataset))\n",
        "\n",
        "    # init model\n",
        "    model = Text2BrainModel(\n",
        "        out_channels=1,\n",
        "        fc_channels=1024,\n",
        "        decoder_filters=256,\n",
        "        pretrained_bert_dir=\"/data3/weiyaonai/project/chat2brain/chat2brain/scibert_scivocab_uncased\",\n",
        "        drop_p=0.6).cuda(args.local_rank)\n",
        "    # model.cuda()\n",
        "    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model) # improved accuracy for small batch sizes\n",
        "\n",
        "    # models sent to different gpu, not models in parallel\n",
        "    model = torch.nn.parallel.DistributedDataParallel(model,\n",
        "                                                      device_ids=[args.local_rank],\n",
        "                                                      output_device=args.local_rank,\n",
        "                                                      find_unused_parameters=True,\n",
        "                                                      broadcast_buffers=True\n",
        "                                                      )\n",
        "\n",
        "    # Optimizer\n",
        "    num_training_steps = len(train_dataset) * 2000\n",
        "    num_warmup_steps = num_training_steps // 3\n",
        "    opt = transformers.AdamW([\n",
        "        {'params': model.module.fc.parameters()},\n",
        "        {'params': model.module.decoder.parameters()},\n",
        "        {'params': model.module.encoder.parameters(), 'lr': 1e-5},\n",
        "    ], lr=3e-2, weight_decay=1e-6)\n",
        "    sched = transformers.get_linear_schedule_with_warmup(opt, num_warmup_steps, num_training_steps)\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"sum\").cuda(args.local_rank)\n",
        "\n",
        "    val_losses = []\n",
        "    val_corrs = []\n",
        "    best_loss = sys.float_info.max\n",
        "    best_corr = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(2000)):\n",
        "        each_dist_train_data_num = ((len(train_dataset) % dist.get_world_size()) + len(\n",
        "            train_dataset)) / dist.get_world_size()\n",
        "\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        val_sampler.set_epoch(epoch)\n",
        "\n",
        "        train_loss, train_corr = train(args.local_rank, model, train_loader, opt, loss_fn, 'aug')\n",
        "        val_loss, val_corr = eval(args.local_rank, model, val_loader, loss_fn, 'aug')\n",
        "\n",
        "        dist.barrier()\n",
        "\n",
        "        if args.local_rank == 0: # only work in local_rank 0 (first gpu)\n",
        "            writer.add_scalar('training loss', train_loss, epoch)\n",
        "            writer.add_scalar('training corr', train_corr, epoch)\n",
        "\n",
        "            writer.add_scalar('validation loss', val_loss, epoch)\n",
        "            writer.add_scalar('validation corr', val_corr, epoch)\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_corrs.append(val_corr)\n",
        "\n",
        "        mean_loss = np.mean(val_losses[-10:])\n",
        "        mean_corr = np.mean(val_corrs[-10:])\n",
        "\n",
        "        if dist.get_rank() == 0:\n",
        "            if (epoch > 2000 * 0.1) and (epoch % 10 == 0):\n",
        "                if mean_loss < best_loss:\n",
        "                    save_checkpoint(model, opt, sched, epoch, \"best_loss.pth\", output_dir)\n",
        "                    best_loss = mean_loss\n",
        "                if mean_corr > best_corr:\n",
        "                    save_checkpoint(model, opt, sched, epoch, \"best_corr.pth\", output_dir)\n",
        "                    best_corr = mean_corr\n",
        "                save_checkpoint(model, opt, sched, epoch, f'checkpoint_{epoch}.pth', output_dir)\n",
        "            sched.step()\n",
        "        save_checkpoint(model, opt, sched, epoch, f'checkpoint_{epoch}.pth', output_dir)\n",
        "\n",
        "    if dist.get_rank() == 0:\n",
        "        writer.close()"
      ],
      "metadata": {
        "id": "QQP8sXQo7ng5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat2Brain_Test"
      ],
      "metadata": {
        "id": "P-i1APeO8Oct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rouge\n",
        "!pip install -q openai\n",
        "!pip install -q evaluate\n",
        "!pip install -q rouge_score"
      ],
      "metadata": {
        "id": "3qNA1mbp8SQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Rf4J5y8JV1S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import openai\n",
        "import evaluate\n",
        "import concurrent.futures\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from rouge import Rouge\n",
        "from rouge.rouge import rouge_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "j39eFWzYV31i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chatgpt"
      ],
      "metadata": {
        "id": "4WyGx_dxqrj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat2brain_gpt(text, near_samples=None, interactive=False, former_good_response=None, former_bad_response=None):\n",
        "  max_tries = 2\n",
        "\n",
        "  dynamic_messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are a brain science expert. Rewrite a research paper title \\\n",
        "       that contains key information about the brain science based on the given original text. \\\n",
        "        Please ensure that your response is concise and does not exceed the length of the research paper title.\"}]\n",
        "  if near_samples is not None:\n",
        "    dynamic_messages.append({\"role\": \"user\", \"content\": \"Here are some examples. Please learn how to write research paper title in these examples, and \\\n",
        "    pay particular attention to the use of brain science concepts in the following example.\"})\n",
        "    for near in near_samples:\n",
        "      aug_path = os.path.join(aug_dir, str(near[\"id\"]) + \".npy\")\n",
        "      aug_npy = np.load(aug_path, allow_pickle=True).tolist()\n",
        "\n",
        "      dynamic_messages.append({\"role\": \"user\", \"content\": \"What are the research paper title that contains key information about the brain science based on the given original text:\\\n",
        "          \\nTEXT:\\n{}\".format(aug_npy[\"Title1\"].replace('\\n', ''))})\n",
        "      dynamic_messages.append({\"role\": \"assistant\", \"content\": \"TITLE:\\n{}\".format(near[\"title\"].replace('\\n', ''))})\n",
        "\n",
        "  for try_number in range(max_tries):\n",
        "    try:\n",
        "      if not interactive:\n",
        "        dynamic_messages.append({\"role\": \"user\", \"content\": \"What are the research paper title that contains key information about the brain science \\\n",
        "         based on the given original text: \\nTEXT: {}\".format(text)})\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=dynamic_messages )\n",
        "      else:\n",
        "        dynamic_messages.append({\"role\": \"user\", \"content\": \"What are the research paper title that contains key information about the brain science \\\n",
        "                        based on the given original text: \\nTEXT: {}\".format(text)})\n",
        "        if former_bad_response is not None:\n",
        "          for index, item in enumerate(former_bad_response):\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"Below is a bad research paper title based on the given original text above:\"})\n",
        "            dynamic_messages.append({\"role\": \"assistant\", \"content\": \"TITLE:\\n{}\".format(item[\"title\"].replace('\\n', ''))})\n",
        "\n",
        "          if len(former_good_response) == 0:\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"Please give another research paper title based on the given original text. \\\n",
        "            It is important to note that your answer should avoid being consistent with the bad research paper title above. \\\n",
        "            Please pay particular attention to the consistent use of phrases in the above examples\"})\n",
        "\n",
        "        if former_good_response is not None:\n",
        "          for index, item in enumerate(former_good_response):\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"Below is an excellent research paper title based on the given original text above:\"})\n",
        "            dynamic_messages.append({\"role\": \"assistant\", \"content\": \"TITLE:\\n{}\".format(item[\"title\"].replace('\\n', ''))})\n",
        "\n",
        "          if len(former_bad_response) == 0:\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"This is an excellent research paper title, \\\n",
        "              please give another research paper title in this format, and do not exceed the length of this research paper title.\\\n",
        "              Please pay particular attention to the consistent use of phrases in the above examples\"})\n",
        "          else:\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"Please give another research paper title based on the given original text above. \\\n",
        "              It is important to note that your answer should avoid being consistent with the bad research paper title above, \\\n",
        "              but should be consistent with the excellent research paper title above, and do not exceed the length of the excellent research paper title. \\\n",
        "              And please pay particular attention to the consistent use of phrases in the above examples\"})\n",
        "\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=dynamic_messages\n",
        "        )\n",
        "      return response['choices'][0]['message']['content']\n",
        "\n",
        "    except openai.error.APIError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('APIError')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.Timeout as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('Timeout')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.APIConnectionError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('APIConnectionError')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.RateLimitError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('RateLimitError')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.InvalidRequestError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('InvalidRequestError')\n",
        "        return -2\n",
        "      else:\n",
        "        time.sleep(0.1)"
      ],
      "metadata": {
        "id": "1UjEwYFyV-XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finding similar examples in the corpus"
      ],
      "metadata": {
        "id": "CJqAWd2AqtmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_near(test_sample, train_data, near_num, rouge_type):\n",
        "  distance_sample = []\n",
        "  for index in train_data.index:\n",
        "    id = int(train_data.at[index, 'id'])\n",
        "    title = train_data.at[index, 'title']\n",
        "\n",
        "    scores = rouge.get_scores(test_sample, title)\n",
        "    distance_sample.append({'id': id, 'score': scores[0][rouge_type]['f'], 'title': title})\n",
        "\n",
        "  sort_dis = sorted(distance_sample, key=lambda x:x.__getitem__('score'), reverse=True)\n",
        "  near_dis = sort_dis[:near_num]\n",
        "\n",
        "  return near_dis"
      ],
      "metadata": {
        "id": "gEG2g1YZWBC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "impressionGPT for Chat2Brain"
      ],
      "metadata": {
        "id": "K48DWF5nq13w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat2brain_test(args):\n",
        "  row_index, row = args\n",
        "\n",
        "  id, title = row['id'], row['title']\n",
        "\n",
        "  text_path = os.path.join(text_dir, str(id) + '.npy')\n",
        "  mask_path = os.path.join(mask_dir, str(id) + 'nii.gz')\n",
        "\n",
        "  outdir = os.path.join('/content/drive/MyDrive/Colab/CoordinateGPT/result/output', str(id) + '.npy')\n",
        "  if os.path.exists(outdir):\n",
        "    final_best_response = np.load(outdir)\n",
        "    final_best_response = final_best_response.tolist()\n",
        "\n",
        "    return row_index, final_best_response\n",
        "\n",
        "  near_sample = compute_near(title, train_meta, near_k_samples, rouge_type)\n",
        "\n",
        "  good_response = []\n",
        "  bad_response = []\n",
        "  former_score = 0\n",
        "\n",
        "  all_response_score, all_response = [], []\n",
        "  try_count = 0\n",
        "\n",
        "  while True:\n",
        "    if len(good_response) == 0 and len(bad_response) == 0:\n",
        "      try:\n",
        "        response = chat2brain_gpt(title, near_samples=near_sample)\n",
        "      except Exception as e:\n",
        "        time.sleep(5)\n",
        "        continue\n",
        "    else:\n",
        "      try:\n",
        "        response = chat2brain_gpt(title, near_samples=near_sample, interactive=interactive, former_good_response=good_reponse, former_bad_response=bad_reponse)\n",
        "      except Exception as e:\n",
        "        time.sleep(5)\n",
        "        continue\n",
        "\n",
        "    if response == -2:\n",
        "      print(\"exceed length, pop 2 similar examples\")\n",
        "      for i in range(2):\n",
        "        near_sample.pop()\n",
        "      continue\n",
        "\n",
        "    response = response.replace('\\n', '')\n",
        "\n",
        "    compare_scores = []\n",
        "    for near_sa in near_sample:\n",
        "      train_index = train_meta[train_meta['id']==near_sa['id']].index\n",
        "      scores = rouge.get_scores([response], [train_meta.at[train_index[0], 'title']])\n",
        "      rouge_score = scores[0][rouge_type]['f']\n",
        "      compare_scores.append(rouge_score)\n",
        "\n",
        "    score = np.mean(np.array(compare_scores))\n",
        "\n",
        "    all_response_score.append(score)\n",
        "    all_response.append(response)\n",
        "    try_count += 1\n",
        "\n",
        "    if score >= rouge_thre and score > former_score:\n",
        "      former_score = score\n",
        "      good_response.clear()\n",
        "      good_response.append({'title': response, 'score': score})\n",
        "\n",
        "    if score < rouge_thre and score < former_score:\n",
        "      if len(bad_response) > 8:\n",
        "        bad_response = bad_response[-8:]\n",
        "      bad_response.append({'title': response, 'score': score})\n",
        "\n",
        "    if try_count > interactive_times:\n",
        "      break\n",
        "\n",
        "  max_score_index = all_response_score.index(max(all_response_score))\n",
        "  max_score = all_response_score[max_score_index]\n",
        "  final_best_response = all_response[max_score_index]\n",
        "\n",
        "  final_best_response = np.array(final_best_response)\n",
        "  np.save(outdir, final_best_response)\n",
        "\n",
        "  return row_index, final_best_response"
      ],
      "metadata": {
        "id": "rDQ4-TxkWC0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main"
      ],
      "metadata": {
        "id": "KysXF1W1q6WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "openai.api_key = '[openai_key]'\n",
        "\n",
        "interactive = True\n",
        "interactive_times = 10 # upper limit on the number of times a single sample can interact with chatgpt\n",
        "rouge_thre = 0.7 # thresholds for determining excellent response\n",
        "near_k_samples = 14 # number of approximate samples\n",
        "rouge_type = 'rouge-1'\n",
        "\n",
        "aug_dir = '/content/drive/MyDrive/Colab/CoordinateGPT/ChatAUG/AUG_1'\n",
        "\n",
        "train_meta = pd.read_csv('/content/drive/MyDrive/Colab/CoordinateGPT/train_meta.csv')\n",
        "val_meta = pd.read_csv('/content/drive/MyDrive/Colab/CoordinateGPT/val_meta.csv')\n",
        "test_meta = pd.read_csv('/content/drive/MyDrive/Colab/CoordinateGPT/test_meta.csv')\n",
        "\n",
        "text_dir = '/content/drive/MyDrive/Colab/CoordinateGPT/ChatAUG/AUG_1'\n",
        "mask_dir = '/content/drive/MyDrive/Colab/CoordinateGPT/neuroquery_brain_maps'\n",
        "save_path = '/content/drive/MyDrive/Colab/CoordinateGPT/result'\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)\n",
        "print(f'Save path: {save_path}')\n",
        "\n",
        "test_meta.drop(test_meta.columns[0], axis=1, inplace=True)\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  result = dict(tqdm(executor.map(chat2brain_test, test_meta.iterrows()), total=len(test_meta)))\n",
        "\n",
        "test_meta['Result'] = test_meta.index.map(result.get)\n",
        "out_file_name = 'test_time{}_thre{}_near{}_{}_num{}.csv'.format(interactive_times, rouge_thre, near_k_samples, rouge_type)\n",
        "output_file = os.path.join(save_path, '{}'.format(out_file_name))\n",
        "test_meta.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "qZy3xI0WWEsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat2Brain_Score"
      ],
      "metadata": {
        "id": "tZ_AJ2HLWHaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "A5jVI4f-WfAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0w29tMHuWdz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "import scipy.stats\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import nibabel as nib\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from argparse import ArgumentParser"
      ],
      "metadata": {
        "id": "lsuPc7hzWOoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_decoder"
      ],
      "metadata": {
        "id": "DjVm5mecq9vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "\n",
        "    def forward(self, input_):\n",
        "        out = self.conv1(input_)\n",
        "        out = self.bn1(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        identity = self.conv1(input_)\n",
        "        residue = self.bn1(identity)\n",
        "        residue = self.act_fn(residue)\n",
        "        residue = self.conv2(residue)\n",
        "        out = identity + residue\n",
        "        out = self.bn2(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransConvResBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn):\n",
        "        super().__init__()\n",
        "        self.trans_conv1 = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, output_padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
        "        self.act_fn = act_fn\n",
        "\n",
        "    def forward(self, input_):\n",
        "        out = self.trans_conv1(input_)\n",
        "        out = self.bn1(out)\n",
        "        out = self.act_fn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImageDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, act_fn=nn.Sigmoid, num_filter=256):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_filter = num_filter\n",
        "        act_fn = nn.Hardtanh(min_val=-6, max_val=6)\n",
        "\n",
        "        self.trans_1 = TransConvResBlock3D(self.in_channels, self.num_filter, act_fn)\n",
        "        self.trans_2 = TransConvResBlock3D(self.num_filter, self.num_filter // 2, act_fn)\n",
        "        self.trans_3 = TransConvResBlock3D(self.num_filter // 2, self.num_filter // 4, act_fn)\n",
        "\n",
        "        self.out = SimpleConvResBlock3D(self.num_filter // 4, self.out_channels, act_fn)\n",
        "\n",
        "    def forward(self, input_):\n",
        "        up_1 = self.trans_1(input_)\n",
        "        up_2 = self.trans_2(up_1)\n",
        "        up_3 = self.trans_3(up_2)\n",
        "\n",
        "        out = self.out(up_3)\n",
        "\n",
        "        return out[:, :, 1:, 1:, 1:]"
      ],
      "metadata": {
        "id": "Y7ZBe2roWshF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_main_with_encoder"
      ],
      "metadata": {
        "id": "Wo3vXEoYrByu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Text2BrainModel(nn.Module):\n",
        "    def __init__(self, out_channels, fc_channels, decoder_filters, pretrained_bert_dir, decoder_act_fn=nn.Sigmoid, drop_p=0.5, decoder_input_shape=[4, 5, 4]):\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.fc_channels = fc_channels\n",
        "        self.decoder_filters = decoder_filters\n",
        "        self.decoder_input_shape = decoder_input_shape\n",
        "        self.drop_p = drop_p\n",
        "\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(pretrained_bert_dir)\n",
        "        self.encoder = transformers.BertModel.from_pretrained(pretrained_bert_dir)\n",
        "        if torch.cuda.is_available():\n",
        "          self.encoder = self.encoder.cuda()\n",
        "\n",
        "        self.fc = nn.Linear(\n",
        "          in_features=768,\n",
        "          out_features=self.decoder_input_shape[0]*self.decoder_input_shape[1]*self.decoder_input_shape[2]*self.fc_channels)\n",
        "        self.dropout = nn.Dropout(self.drop_p)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.decoder = ImageDecoder(in_channels=self.fc_channels, out_channels=1, num_filter=self.decoder_filters, act_fn=decoder_act_fn)\n",
        "\n",
        "\n",
        "    def forward(self, texts):\n",
        "        batch = [self._tokenize(x) for x in texts]\n",
        "\n",
        "        in_mask = self._pad_mask(batch, batch_first=True)\n",
        "        in_ = pad_sequence(batch, batch_first=True)\n",
        "        if torch.cuda.is_available():\n",
        "          in_ = in_.cuda()\n",
        "          in_mask = in_mask.cuda()\n",
        "\n",
        "        _, embedding = self.encoder(in_, attention_mask=in_mask)\n",
        "\n",
        "        x = self.dropout(embedding)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        decoder_tensor_shape = [-1, self.fc_channels] + self.decoder_input_shape\n",
        "        x = x.view(decoder_tensor_shape)\n",
        "\n",
        "        out = self.decoder(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        return self.tokenizer.encode(text, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=512).squeeze(0)\n",
        "\n",
        "\n",
        "    def _pad_mask(self, sequences, batch_first=False):\n",
        "        ret = [torch.ones(len(s)) for s in sequences]\n",
        "        return pad_sequence(ret, batch_first=batch_first)"
      ],
      "metadata": {
        "id": "Dqtu_RGHWwXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "args"
      ],
      "metadata": {
        "id": "7ARRniF9rETH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_args():\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--gpus\", type=str,\n",
        "                        default=\"0, 1, 2, 3\",\n",
        "                        help=\"Which gpus to use?\")\n",
        "\n",
        "    parser.add_argument(\"--ver\", type=str,\n",
        "                        default=\"neuroquery\",\n",
        "                        help=\"Additional string for the name of the file\")\n",
        "\n",
        "    parser.add_argument(\"--train_csv\",\n",
        "                        type=str,\n",
        "                        help=\"Path to the csv containing the training articles data\")\n",
        "\n",
        "    parser.add_argument(\"--val_csv\",\n",
        "                        type=str,\n",
        "                        help=\"Path to the csv containing the validation articles data\")\n",
        "\n",
        "    parser.add_argument(\"--images_dir\",\n",
        "                        type=str,\n",
        "                        help=\"Directory containing activation maps, should be of size (40, 48, 40)\")\n",
        "\n",
        "    parser.add_argument(\"--pretrained_bert_dir\",\n",
        "                        type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/scibert_scivocab_uncased\",\n",
        "                        help=\"Directory containing pretrained BERT model\")\n",
        "\n",
        "    parser.add_argument(\"--pretrained_tokenizer_dir\",\n",
        "                        type=str,\n",
        "                        help=\"Directory containing pretrained tokenizer\")\n",
        "\n",
        "    parser.add_argument(\"--mask_file\",\n",
        "                        type=str,\n",
        "                        help=\"Brain mask file\")\n",
        "\n",
        "    parser.add_argument(\"--save_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/Chat2Brain_checkpoint/\",\n",
        "                        help=\"Path to the output directory\")\n",
        "\n",
        "    parser.add_argument(\"--save_test_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/Chat2Brain_test/\",\n",
        "                        help=\"Path to the output directory\")\n",
        "\n",
        "    parser.add_argument(\"--mask_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/data/brain_maps/neuroquery/\",\n",
        "                        help=\"Path to the mask directory\")\n",
        "\n",
        "    parser.add_argument(\"--text_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/data/ChatAUG/\",\n",
        "                        help=\"Path to the text directory\")\n",
        "\n",
        "    parser.add_argument(\"--metadata_dir\", type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/data-neuroquery_version-1_metadata.tsv.gz\",\n",
        "                        help=\"Path to the metadata directory\")\n",
        "\n",
        "    parser.add_argument(\"--n_fc_channels\",\n",
        "                        type=int,\n",
        "                        default=1024,\n",
        "                        help=\"Base number of channels in the FC layer\")\n",
        "\n",
        "    parser.add_argument(\"--n_decoder_channels\",\n",
        "                        type=int,\n",
        "                        default=256,\n",
        "                        help=\"Base number of channels in the image decoder\")\n",
        "\n",
        "    parser.add_argument(\"--n_output_channels\",\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help=\"Number of output channels\")\n",
        "\n",
        "    parser.add_argument(\"--lr\",\n",
        "                        type=float,\n",
        "                        default=3e-2,\n",
        "                        help=\"Learning rate\")\n",
        "\n",
        "    parser.add_argument(\"--weight_decay\",\n",
        "                        type=float,\n",
        "                        default=1e-6,\n",
        "                        help=\"Weight decay of the optimizer\")\n",
        "\n",
        "    parser.add_argument(\"--drop_p\",\n",
        "                        type=float,\n",
        "                        default=0.6,\n",
        "                        help=\"Dropout proportion for FC layer\")\n",
        "\n",
        "    parser.add_argument(\"--epochs\",\n",
        "                        type=int,\n",
        "                        default=550,\n",
        "                        help=\"Training epochs\")\n",
        "\n",
        "    parser.add_argument(\"--seed\",\n",
        "                        type=int,\n",
        "                        default=28)\n",
        "\n",
        "    parser.add_argument(\"--random_seed\",\n",
        "                        type=int,\n",
        "                        default=60)\n",
        "\n",
        "    parser.add_argument(\"--split\",\n",
        "                        type=list,\n",
        "                        default=[6, 2, 2])\n",
        "\n",
        "    parser.add_argument(\"--checkpoint_file\",\n",
        "                        type=str,\n",
        "                        default=\"/disk1/wyn/workshop/ChatGPT/text2brain-main/Chat2Brain_checkpoint/neuroquery_title_fc1024_dec256_lr0.03_decay1e-06_drop0.6_seed28/checkpoint_1450.pth\",\n",
        "                        help=\"Path to the checkpoint file to be loaded into the model\")\n",
        "\n",
        "    parser.add_argument(\"--checkpoint_interval\",\n",
        "                        type=int,\n",
        "                        default=10,\n",
        "                        help=\"Number of epochs between saved checkpoints\")\n",
        "\n",
        "    parser.add_argument(\"--batch_size\",\n",
        "                        type=int,\n",
        "                        default=24,\n",
        "                        help=\"Batch size\")\n",
        "\n",
        "    parser.add_argument(\"--Scaling_factor\",\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help=\"Scaling factor\")\n",
        "\n",
        "    parser.add_argument(\"--phrase\",\n",
        "                        type=str,\n",
        "                        default=None,\n",
        "                        help=\"Input phrase for prediction\")\n",
        "\n",
        "    parser.add_argument(\"--source\",\n",
        "                        type=str,\n",
        "                        default=\"title\",\n",
        "                        help=\"Source type\")\n",
        "\n",
        "    return parser.parse_args()"
      ],
      "metadata": {
        "id": "gLlheV1NWybh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "8AkveH_WrF3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x):\n",
        "  x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def get_score(result_r, result_o, gt):\n",
        "  auc_all_r = 0\n",
        "  dice_all_r = 0\n",
        "  pval_all_r = 0\n",
        "\n",
        "  auc_all_o = 0\n",
        "  dice_all_o = 0\n",
        "  pval_all_o = 0\n",
        "\n",
        "  for i in range(len(gt)):\n",
        "    i += 1\n",
        "    true = gt[i-1].flatten()\n",
        "    pred_r = result_r[i-1].flatten()\n",
        "    pred_o = result_o[i-1].flatten()\n",
        "\n",
        "    true_1 = true.copy()\n",
        "    true_1 = norm(true_1)\n",
        "\n",
        "    pred_1_r = pred_r.copy()\n",
        "    pred_1_o = pred_o.copy()\n",
        "\n",
        "    pred_1_r = norm(pred_1_r)\n",
        "    pred_1_o = norm(pred_1_o)\n",
        "\n",
        "    auc_r = roc_auc_score(true_1.astype('int'), pred_1_r)\n",
        "    auc_all_r += auc_r\n",
        "\n",
        "    auc_o = roc_auc_score(true_1.astype('int'), pred_1_o)\n",
        "    auc_all_o += auc_o\n",
        "\n",
        "    dice_r = f1_score(true_1.astype('int'), pred_1_r.astype('int'))\n",
        "    dice_all_r += dice_r\n",
        "\n",
        "    dice_o = f1_score(true_1.astype('int'), pred_1_o.astype('int'))\n",
        "    dice_all_o += dice_o\n",
        "\n",
        "    t_r, pval_r = scipy.stats.ttest_ind(true, pred_r)\n",
        "    pval_all_r += pval_r\n",
        "\n",
        "    t_o, pval_o = scipy.stats.ttest_ind(true, pred_o)\n",
        "    pval_all_o += pval_o\n",
        "\n",
        "    writer.add_scalars(\"test_score\", {\n",
        "        'Auc_r': auc_r,\n",
        "        'Dice_r': dice_r,\n",
        "        'Pval_r': pval_r,\n",
        "        'Auc_o': auc_o,\n",
        "        'Dice_o': dice_o,\n",
        "        'Pval_o': pval_o,\n",
        "    }, i)\n",
        "\n",
        "\n",
        "  mean_auc_r = auc_all_r / i\n",
        "  mean_dice_r = dice_all_r / i\n",
        "  mean_pval_r = pval_all_r / i\n",
        "\n",
        "  mean_auc_o = auc_all_o / i\n",
        "  mean_dice_o = dice_all_o / i\n",
        "  mean_pval_o = pval_all_o / i\n",
        "  print(mean_auc_r, mean_dice_r, mean_pval_r, mean_auc_o, mean_dice_o, mean_pval_o)\n",
        "  return mean_auc_r, mean_dice_r, mean_pval_r, mean_auc_o, mean_dice_o, mean_pval_o"
      ],
      "metadata": {
        "id": "pIxkf7iTZbUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main"
      ],
      "metadata": {
        "id": "b7jmFb4ErIhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    # init\n",
        "    args = init_args()\n",
        "\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
        "\n",
        "    output_dir = os.path.join(args.save_test_dir, 'title')\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    else:\n",
        "        print(f'Output dir exists: {output_dir}')\n",
        "\n",
        "    writer = SummaryWriter(os.path.join(output_dir, \"test_time10_thre0.7_near14_rouge-1_num_logs\"))\n",
        "\n",
        "    # load Data\n",
        "    test_meta = pd.read_csv('/disk1/wyn/workshop/ChatGPT/text2brain-main/Result/test_time10_thre0.7_near14_rouge-1_num10.csv')\n",
        "    brain_map_dir = '/disk1/wyn/workshop/ChatGPT/text2brain-main/data/brain_maps/neuroquery'\n",
        "    output = '/disk1/wyn/workshop/ChatGPT/text2brain-main/Result/output/'\n",
        "\n",
        "    # init model\n",
        "    model = Text2BrainModel(\n",
        "        out_channels=1,\n",
        "        fc_channels=args.n_fc_channels,\n",
        "        decoder_filters=args.n_decoder_channels,\n",
        "        pretrained_bert_dir=args.pretrained_bert_dir,\n",
        "        drop_p=args.drop_p)\n",
        "    model.cuda()\n",
        "\n",
        "    # loading checkpoint\n",
        "    state_dict = torch.load('/disk1/wyn/workshop/ChatGPT/text2brain-main/Chat2Brain_checkpoint/aug_loss.pth')['state_dict']\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # test\n",
        "    gt = []\n",
        "    r = []\n",
        "    o = []\n",
        "\n",
        "    for index, row in test_meta.iterrows():\n",
        "        id = row['id']\n",
        "\n",
        "        text_r = row['Result']\n",
        "        text_r = text_r.replace('TITLE:', '')\n",
        "        text_r = text_r.replace('\\n', '')\n",
        "        text_r = text_r.lstrip() # optimized text by impressionGPT\n",
        "\n",
        "        text_o = row['title'] # original text\n",
        "\n",
        "        brain_map = nib.load(os.path.join(brain_map_dir, str(id) + '.nii.gz'))\n",
        "        brain_map = brain_map.get_fdata()\n",
        "        brain_map = brain_map[3:-3, 3:-4, :-6]\n",
        "        brain_map = brain_map / np.max(brain_map)\n",
        "        brain_map = np.nan_to_num(brain_map, copy=False)\n",
        "\n",
        "        gt.append(brain_map)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            predict_r = model([text_r])\n",
        "            predict_o = model([text_o])\n",
        "\n",
        "        predict_r = predict_r.cpu().detach().numpy()\n",
        "        predict_o = predict_o.cpu().detach().numpy()\n",
        "\n",
        "        predict_r = np.squeeze(predict_r)\n",
        "        predict_r = predict_r / np.max(predict_r)\n",
        "        predict_r = np.nan_to_num(predict_r, copy=False)\n",
        "\n",
        "        predict_o = np.squeeze(predict_o)\n",
        "        predict_o = predict_o / np.max(predict_o)\n",
        "        predict_o = np.nan_to_num(predict_o, copy=False)\n",
        "\n",
        "        r.append(predict_r)\n",
        "        o.append(predict_o)\n",
        "\n",
        "\n",
        "\n",
        "    gt = np.array(gt)\n",
        "    o = np.array(o)\n",
        "    r = np.array(r)\n",
        "\n",
        "    np.save(os.path.join(output, 'gt.npy'), gt)\n",
        "    np.save(os.path.join(output, 'o.npy'), o)\n",
        "    np.save(os.path.join(output, 'r.npy'), r)\n",
        "\n",
        "    mean_auc_r, mean_dice_r, mean_pval_r, mean_auc_o, mean_dice_o, mean_pval_o = get_score(r, o, gt)\n",
        "    writer.close()"
      ],
      "metadata": {
        "id": "iJlfPWlYZeSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "cFTr7F9caCg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ys0s2t0WaKMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_index = 2\n",
        "thresholds = 0.8\n",
        "Res_result = np.load('/content/drive/MyDrive/Colab/CoordinateGPT/result/r.npy')\n",
        "\n",
        "m, n, k = np.where(Res_result[sample_index]>thresholds)\n",
        "fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n",
        "scatter = ax.scatter(m, n, k)"
      ],
      "metadata": {
        "id": "nBevhevkaHip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}